{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART作为二叉决策树，由特征选择、树的生成及剪枝组成，既可以分类，也可以回归。\n",
    "分类时：基尼指数最小化。\n",
    "回归时：平方误差最小化。\n",
    "数据类型：标值型，连续型。连续型分类时采取“二分法”， 取中间值进行左右子树的划分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三种决策树的区别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3：特征划分基于信息增益\n",
    "C4.5：特征划分基于信息增益比\n",
    "CART：特征划分基于基尼指数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类树 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征A有N个取值，将每个取值作为分界点，将数据D分为两类，然后计算基尼指数Gini(D,A), 选择基尼指数小的特征A的取值。然后对于每个特征在计算基尼指数，最后得到最佳的特征的最佳取值作为分支点。\n",
    "基尼指数表示数据D的不纯度，基尼指数越小不纯度越小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART算法由以下两步组成：\n",
    "\n",
    "决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大;\n",
    "\n",
    "决策树剪枝：用验证数据集对已生成的树进行剪枝并选择最优子树，这时损失函数最小作为剪枝的标准。\n",
    "\n",
    "CART决策树的生成就是递归地构建二叉决策树的过程。CART决策树既可以用于分类也可以用于回归。对分类树而言，CART用Gini系数最小化准则来进行特征选择，生成二叉树。 CART生成算法如下：\n",
    "\n",
    "输入：训练数据集D，停止计算的条件：\n",
    "\n",
    "输出：CART决策树。\n",
    "\n",
    "根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：\n",
    "\n",
    "设结点的训练数据集为D，计算现有特征对该数据集的Gini系数。此时，对每一个特征A，对其可能取的每个值a，根据样本点对A=a的测试为“是”或 “否”将D分割成D1和D2两部分，计算A=a时的Gini系数。\n",
    "\n",
    "在所有可能的特征A以及它们所有可能的切分点a中，选择Gini系数最小的特征及其对应的切分点作为最优特征与最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。对两个子结点递归地调用步骤l~2，直至满足停止条件。\n",
    "\n",
    "生成CART决策树。\n",
    "\n",
    "算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的Gini系数小于预定阈值(样本基本属于同一类)，或者没有更多特征。\n",
    "\n",
    "while (当前节点”不纯“)  \n",
    "\t(1)计算当前节点的类别信息熵Info(D) （以类别取值计算）  \n",
    "\t(2)计算当前节点各个属性的信息熵Info(Ai) （以属性取值下的类别取值计算）  \n",
    "\t(3)计算各个属性的信息增益Gain(Ai)=Info(D)-Info(Ai)  \n",
    "\t(4)计算各个属性的分类信息度量H(Ai) （以属性取值计算）  \n",
    "\t(5)计算各个属性的信息增益率IGR(Ai)=Gain(Ai)/H(Ai)  \n",
    "end while  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 补充代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'声音': {'细': '女', '粗': {'头发': {'短': '男', '长': '女'}}}}\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "import operator\n",
    "\n",
    "def calcShannonEnt(dataSet):  # 计算数据的熵(entropy)\n",
    "    numEntries=len(dataSet)  # 数据条数\n",
    "    labelCounts={}\n",
    "    for featVec in dataSet:\n",
    "        currentLabel=featVec[-1] # 每行数据的最后一个字（类别）\n",
    "        if currentLabel not in labelCounts.keys():\n",
    "            labelCounts[currentLabel]=0\n",
    "        labelCounts[currentLabel]+=1  # 统计有多少个类以及每个类的数量\n",
    "    shannonEnt=0\n",
    "    for key in labelCounts:\n",
    "        prob=float(labelCounts[key])/numEntries # 计算单个类的熵值\n",
    "        shannonEnt-=prob*log(prob,2) # 累加每个类的熵值\n",
    "    return shannonEnt\n",
    "\n",
    "def createDataSet1():    # 创造示例数据\n",
    "    dataSet = [['长', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['短', '粗', '男'],\n",
    "               ['长', '细', '女'],\n",
    "               ['短', '细', '女'],\n",
    "               ['短', '粗', '女'],\n",
    "               ['长', '粗', '女'],\n",
    "               ['长', '粗', '女']]\n",
    "    labels = ['头发','声音']  #两个特征\n",
    "    return dataSet,labels\n",
    "\n",
    "def splitDataSet(dataSet,axis,value): # 按某个特征分类后的数据\n",
    "    retDataSet=[]\n",
    "    for featVec in dataSet:\n",
    "        if featVec[axis]==value:\n",
    "            reducedFeatVec =featVec[:axis]\n",
    "            reducedFeatVec.extend(featVec[axis+1:])\n",
    "            retDataSet.append(reducedFeatVec)\n",
    "    return retDataSet\n",
    "\n",
    "def chooseBestFeatureToSplit(dataSet):  # 选择最优的分类特征\n",
    "    numFeatures = len(dataSet[0])-1\n",
    "    baseEntropy = calcShannonEnt(dataSet)  # 原始的熵\n",
    "    bestInfoGain = 0\n",
    "    bestFeature = -1\n",
    "    for i in range(numFeatures):\n",
    "        featList = [example[i] for example in dataSet]\n",
    "        uniqueVals = set(featList)\n",
    "        newEntropy = 0\n",
    "        for value in uniqueVals:\n",
    "            subDataSet = splitDataSet(dataSet,i,value)\n",
    "            prob =len(subDataSet)/float(len(dataSet))\n",
    "            newEntropy +=prob*calcShannonEnt(subDataSet)  # 按特征分类后的熵\n",
    "        infoGain = baseEntropy - newEntropy  # 原始熵与按特征分类后的熵的差值\n",
    "        if (infoGain>bestInfoGain):   # 若按某特征划分后，熵值减少的最大，则次特征为最优分类特征\n",
    "            bestInfoGain=infoGain\n",
    "            bestFeature = i\n",
    "    return bestFeature\n",
    "\n",
    "def majorityCnt(classList):    #按分类后类别数量排序，比如：最后分类为2男1女，则判定为男；\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote]+=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]\n",
    "\n",
    "def createTree(dataSet,labels):\n",
    "    classList=[example[-1] for example in dataSet]  # 类别：男或女\n",
    "    if classList.count(classList[0])==len(classList):\n",
    "        return classList[0]\n",
    "    if len(dataSet[0])==1:\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat=chooseBestFeatureToSplit(dataSet) #选择最优特征\n",
    "    bestFeatLabel=labels[bestFeat]\n",
    "    myTree={bestFeatLabel:{}} #分类结果以字典形式保存\n",
    "    del(labels[bestFeat])\n",
    "    featValues=[example[bestFeat] for example in dataSet]\n",
    "    uniqueVals=set(featValues)\n",
    "    for value in uniqueVals:\n",
    "        subLabels=labels[:]\n",
    "        myTree[bestFeatLabel][value]=createTree(splitDataSet\\\n",
    "                            (dataSet,bestFeat,value),subLabels)\n",
    "    return myTree\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    dataSet, labels=createDataSet1()  # 创造示列数据\n",
    "    print(createTree(dataSet, labels))  # 输出决策树模型结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
